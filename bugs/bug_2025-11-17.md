# Bug Report — Snapshot 2025-11-17_1446

Source artifacts:
- Database: `2025-11-17_1446/testing2.sqlite`
- Logs: `2025-11-17_1446/log.jsonl` and `full_log.jsonl`

## 1. Take-profit scaling never updates (scaled_orders uniqueness trap)

**Summary**
- Repeated attempts to resize the take-profit (TP) order for deal `2386527036` hit `UNIQUE constraint failed: scaled_orders...` and the worker drops the task, so Hyperliquid keeps the original 155 DOGE TP until the very end even though the bot raises the target size with each averaging order.

**Evidence**
- First failure: `2025-11-17_1446/log.jsonl:450` — full log entry  
  `{"time":"2025-11-17T13:38:11.945714+01:00","level":"WARN","msg":"cannot apply scaling","engine":{"deal-id":2386527036,"bot-id":16598267,"botevent-id":3940649977,"bot-event-id":3940649977,"venue":"hyperliquid:testing","error":"record scaled order: constraint failed: UNIQUE constraint failed: scaled_orders.venue_id, scaled_orders.wallet, scaled_orders.order_id (1555)"}}`.
- Identical failures repeat at lines `484`, `706`, `729`, `839`, `880`, `914`, `949`.
- Database still holds only the initial entry for the TP order: `scaled_orders` row `0x00fd44fb8e3f833ceae18bf953eb77c8#0` with `scaled_size = 155`.
- `threecommas_botevents` shows the bot requesting progressively larger TPs (nine SELL botevents for the same cloid).

**Impact**
- Position grows to 3,754 DOGE while the exchange only has a 155 DOGE TP.
- Any restart would continue to believe scaling already happened (row exists) while it actually failed.

**Likely cause**
- Scaling recorder treats each TP adjustment as a brand-new order and inserts into `scaled_orders` with the same PK `<venue>/<wallet>/<order_id>`. On the first insert (the original TP) the row is created; subsequent inserts hit the PK and the worker aborts instead of updating.

**Reproduction (from snapshot setup)**
1. Start recomma against the provided DB snapshot so the bot `16598267` already owns deal `2386527036`.
2. Allow engine to poll 3Commas once new averaging orders arrive; it will enqueue `emit modify` for botevent `3940649977`.
3. Observe worker hitting the constraint and dropping the work, leaving Hyperliquid TP untouched (line 450 onwards).

**Next steps**
1. Change scaling persistence to upsert/update the existing row (or version rows by botevent) so resize requests can progress.
2. Add tests around repeated TP updates to ensure scaling logic can update the same cloid multiple times.

## 2. Hyperliquid modify succeeds remotely but we treat it as failure

**Summary**
- Around 14:45 the engine finally emits the TP resize (target 1,877 DOGE) and Hyperliquid reports the new size, yet the emitter logs “could not modify order … missing response.data field” and forgets the work. Storage is never updated, so recomma still believes the TP is 155 DOGE and that outstanding sells are only 310.

**Evidence**
- Filltracker logs “modified take profit” for both venues at `log.jsonl:983` and `:987`.
- Immediately afterwards, emitter logs the full warning (example from `log.jsonl:990`):  
  `{"time":"2025-11-17T14:45:46.663949+01:00","level":"WARN","msg":"could not modify order","hyperliquid":{"emitter":{"venue":"hyperliquid:testing","wallet":"0x6a087a6bf00ad488594e416d14073264f935b48e","orderid":"0x00fd44fb8e3f833ceae18bf953eb77c8","venue":"hyperliquid:default","bot-event":{"RowID":0,"CreatedAt":"2025-11-17T11:52:24.932Z","Action":"Placing","Coin":"DOGE","Type":"SELL","Status":"Active","Price":0.16571,"Size":155,"OrderType":"Take Profit","OrderSize":0,"OrderPosition":0,"QuoteVolume":25.68505,"QuoteCurrency":"USDT","IsMarket":false,"Profit":0,"ProfitCurrency":"","ProfitUSD":0,"ProfitPercentage":0,"Text":"Placing TakeProfit trade.  Price: 0.16571 USDT Size: 25.68505 USDT (155.0 DOGE), the price should rise for 2.21% to close the trade"},"error":"failed to modify order: missing response.data field in successful response","action":{"Oid":null,"Cloid":{"Value":"0x00fd44fb8e3f833ceae18bf953eb77c8"},"Order":{"Coin":"DOGE","IsBuy":false,"Price":0.16571,"Size":1877,"ReduceOnly":true,"OrderType":{"limit":{"tif":"Gtc"}},"ClientOrderID":"0x00fd44fb8e3f833ceae18bf953eb77c8"}}}}}`.
- `DEBUG` lines `998`, `1016`, `1036`, `1058`, `1066`, `1088` repeat the same full message and include the “error submitting order, forgetting” payloads.
- Hyperliquid status snapshots (`testing2.sqlite -> hyperliquid_status_history`) show the order at 1,877 DOGE from `2025-11-17 13:45:46` until cancellation at `13:46:23`.
- `hyperliquid_submissions` still contains only the original 155 DOGE create payload; `modify_payloads` is `[]`.
- Engine state remains stale: final fill snapshot at `log.jsonl:989` still claims `outstanding_sells: 310`.

**Impact**
- If recomma restarted, it would recreate a TP at 155 DOGE even though Hyperliquid already has a 1,877 DOGE order, leading to double orders or deletes of the wrong size.
- Monitoring dashboards and API clients think there is only 310 DOGE of sell coverage.

**Likely cause**
- Hyperliquid modify endpoint sometimes omits `response.data` in a successful response. Our client interprets that as an error, returns failure, and the emitter forgets the work without persisting the new payload or reconciling.

**Reproduction**
1. Using the same snapshot, wait until all nine averaging orders are filled (net size ~3,754 DOGE).
2. Engine emits the resize to 1,877 DOGE; Hyperliquid accepts it (status feed shows new size).
3. Because the response body lacks `response.data`, the emitter logs the warning above and forgets the work, leaving storage stale.

**Next steps**
1. Treat “success without response.data” as success (or fetch the order immediately to confirm) so we do not discard a live resize.
2. Persist modify payloads for both the primary venue and aliases, and refresh `hyperliquid_submissions` rows to match the new size.
3. Trigger a filltracker reconciliation after a perceived failure to force a status pull and align outstanding_sells with the exchange.

## 3. Alias venue status rows without submissions

**Summary**
- `hyperliquid_status_history` contains rows for `venue_id = hyperliquid:default`, but `hyperliquid_submissions` has zero rows for that venue. The schema declares a FK from status rows back to submissions, so the DB should never allow this state (but the snapshot has it).

**Evidence**
- `sqlite> select count(*) from hyperliquid_submissions where venue_id='hyperliquid:default';` → `0`.
- `sqlite> select count(*) from hyperliquid_status_history where venue_id='hyperliquid:default';` → `45`.
- Example row (order `0x00fd44…029f`): status shows both default and testing venue entries, but only testing exists in submissions.

**Impact**
- After a restart we cannot replay or reconcile alias orders because we lost the original submission payload.
- Violates FK assumptions (either the FK is disabled or the status rows were inserted incorrectly), risking cascade deletes or mismatched state.

**Likely cause**
- When emitting to both the primary venue and its alias, we only persist the submission for the primary (`hyperliquid:testing`). Status fan-out, however, duplicates events for both venues, so alias entries exist without backing submission rows.

**Reproduction**
1. Open `2025-11-17_1446/testing2.sqlite`.
2. Run `select count(*) from hyperliquid_submissions where venue_id='hyperliquid:default';` (result `0`).
3. Run `select count(*) from hyperliquid_status_history where venue_id='hyperliquid:default';` (result `45`), proving status rows exist without submissions.

**Next steps**
1. Ensure submissions are recorded for every venue identifier we emit to (including aliases) before status rows are inserted.
2. Add a migration or repair tool that backfills missing alias submissions from existing payloads/status snapshots so rebuilds can succeed.
